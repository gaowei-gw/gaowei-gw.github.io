---
layout:     post
title:      机器学习概览
subtitle:   机器学习
date:       2020-01-06
author:     KaithyXu
header-img: img/robot.jpg
catalog: true
tags:
    - Java
---

## 机器学习概览


### 惊世一役

2016年，当Google的AlphaGo以4:1战胜世界围棋最强高手李世石时，所有人都知道人工智能火了，虽然李世石的惊天一挖，检测到了AI的盲点，但是不可否认，在围棋方面，我们人为的造就一个“神”。随着互联网技术的迅速发展，产业的变革不再像以往那般缓慢，瞬息万变间，旧的皇帝倒下，新的帝王站起来，当TB甚至PB级的数据出现时，数据就已不再称为机器学习发展的制约，各种算法近年来的不断涌现，进一步促进了机器学习的发展。

### 定义
机器学习是利用计算机算法和统计模型是计算机系统使用，逐步提高完成特定任务的能力。

机器学习建立样本数据的数学模型，称为“ 训练数据 ”，以便在不明确编程以执行任务的情况下进行预测或决策。机器学习算法用于电子邮件过滤，网络入侵者检测和计算机视觉的应用，开发用于执行任务的特定指令的算法是不可行的。机器学习与计算统计密切相关，计算统计侧重于使用计算机进行预测。数学优化的研究为机器学习领域提供了方法，理论和应用领域。数据挖掘是机器学习中的一个研究领域，侧重于通过无监督学习进行探索性数据分析。在跨业务问题的应用中，机器学习也被称为预测分析。

### 分类
一. 根据学习方式的差异，机器学习可以分为四种：
1. 监督学习，利用已知类别的样本作为训练集，通过不断的调整分类器的参数以及重复训练，找到给定训练数据集中的某种模式或规律，最终用训练好的模型对新数据进行分类与预测，常见的监督学习算法包括回归分析和统计分类等。
2. 非监督学习，无需对训练样本进行标记，通过不断的训练，找到数据集中隐含的某种结构，获取样本数据的结构特征，并最终对未知的数据进行预测。典型的算法有自动编码器，受限玻尔兹曼机，深度置信网络等。
3. 半监督学习，监督与非监督学习的结合，在训练阶段使用的是为标记的数据和已标记的数据，不仅要学习样本元素之间的结构关系，也要输出分类模型进行预测
4. 强化学习，强调如何基于环境二行动，以获取最大化的预期利益，与标准的监督式学习之间的区别在于，强化学习不需要出现正确的输入/输出对，也不需要精确矫正次优化的行为，其根据e专注于在线规划，需要在探索(在未知领域)和遵从(现有知识)之间找到平衡

二. 根据学习策略的不同，可分为两类：
1. 模拟人脑的机器学习
    (1). 符号学习
    (2). 神经网络学习
2. 直接采用数学方法的机器学习。
    (1). 统计机器学习，三要素：模型+策略+算法

三. 基于学习方法，可分为四类:
1. 归纳学习
2. 演绎学习
3. 类比学习
4. 分析学习

四. 基于数据形式，可分为两类，结构化学习以及非结构化学习

五. 基于学习目标，可分为五类:
1. 概念学习
2. 规则学习
3. 函数学习
4. 类别学习
5. 贝叶斯网络学习

### 常见的机器学习算法
* 构造间隔理论分布：聚合分析和模式识别
    * 人工神经网络
    * 决策树
    * 感知机
    * 支持向量机
    * 集成学习AdaBoost
    * 降维语度量学习
    * 聚类
    * 贝叶斯分类器
* 构造条件概率
    * 高斯过程回归
    * 线性判别分析
    * 最近邻居法
    * 径向基函数核
* 通过再生模型构造概率密度函数
    * 最大期望算法
    * 概率图模型：包括贝叶斯网核Markov随机场
    * Generative Topographic Mapping
* 近似推断技术
    * 马尔可夫链
    * 蒙特卡罗方法
    * 变分法
* 最优化

### 机器学习损失函数
1. 0-1损失函数
![](https://latex.codecogs.com/gif.latex?L%28y%2Cf%28X%29%29%3D%5Cbegin%7BBmatrix%7D%200%2C%20%26%20%5Ctext%7By%20%3D%20f%28x%29%7D%5C%5C%201%2C%20%26%20%7By%5Cneq%20f%28x%29%7D%20%5Cend%7BBmatrix%7D)
2.绝对值损失函数
![](https://latex.codecogs.com/gif.latex?L%28y%2Cf%28x%29%29%3D%7Cy-f%28x%29%7C)
3. 平方损失函数
![](https://latex.codecogs.com/gif.latex?L%28y%2Cf%28x%29%29%3D%28y-f%28x%29%29%5E2)
4. log对数损失函数
![](https://latex.codecogs.com/gif.latex?L%28y%2Cf%28x%29%29%3Dlog%281&plus;e%5E%7B-yf%28x%29%7D%29)
5. 指数损失函数
![](https://latex.codecogs.com/gif.latex?L%28y%2Cf%28x%29%29%3Dexp%28-yf%28x%29%29)
6. Hinge损失函数
![](https://latex.codecogs.com/gif.latex?L%28w%2Cb%29%3Dmax%5C%7B0%2C1-yf%28x%29%5C%7D)

[参考文章1](https://www.csuldw.com/2016/03/26/2016-03-26-loss-function/)

[参考文章2](https://www.cnblogs.com/lliuye/p/9549881.html)

### 机器学习优化方法
梯度下降是最常用的优化方法之一，它使用梯度的反方向$\nabla_\theta J(\theta)$更新参数$\theta$，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. 
1. (全量)梯度下降
![](https://latex.codecogs.com/gif.latex?%5Ctheta%3D%5Ctheta-%5Ceta%5Cnabla_%5Ctheta%20J%28%5Ctheta%29)
2. 随机梯度下降
![](https://latex.codecogs.com/gif.latex?%5Ctheta%3D%5Ctheta-%5Ceta%5Cnabla_%5Ctheta%20J%28%5Ctheta%3Bx%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29)
3. 小批量梯度下降
![](https://latex.codecogs.com/gif.latex?%5Ctheta%3D%5Ctheta-%5Ceta%5Cnabla_%5Ctheta%20J%28%5Ctheta%3Bx%5E%7B%28i%3Ai&plus;n%29%7D%2Cy%5E%7B%28i%3Ai&plus;n%29%7D%29)
4. 引入动量的梯度下降
![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bcases%7D%20v_t%3D%5Cgamma%20v_%7Bt-1%7D&plus;%5Ceta%20%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%5C%5C%20%5Ctheta%3D%5Ctheta-v_t%20%5Cend%7Bcases%7D)
5. 自适应学习率的Adagrad算法
![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bcases%7D%20g_t%3D%20%5Cnabla_%5Ctheta%20J%28%5Ctheta%29%20%5C%5C%20%5Ctheta_%7Bt&plus;1%7D%3D%5Ctheta_%7Bt%2Ci%7D-%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7BG_t&plus;%5Cvarepsilon%7D%7D%20%5Ccdot%20g_t%20%5Cend%7Bcases%7D)
6. 牛顿法
![](https://latex.codecogs.com/gif.latex?%5Ctheta_%7Bt&plus;1%7D%3D%5Ctheta_t-H%5E%7B-1%7D%5Cnabla_%5Ctheta%20J%28%5Ctheta_t%29)

[参考文章](https://blog.csdn.net/fishmai/article/details/52510826)

### 机器学习的评价指标
1. MSE(Mean Squared Error)
![](https://latex.codecogs.com/gif.latex?MSE%28y%2Cf%28x%29%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y-f%28x%29%29%5E2)
2. MAE(Mean Absolute Error)
![](https://latex.codecogs.com/gif.latex?MSE%28y%2Cf%28x%29%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Cy-f%28x%29%7C)
3. RMSE(Root Mean Squard Error)
![](https://latex.codecogs.com/gif.latex?RMSE%28y%2Cf%28x%29%29%3D%5Cfrac%7B1%7D%7B1&plus;MSE%28y%2Cf%28x%29%29%7D)
4. Top-k准确率
![](https://latex.codecogs.com/gif.latex?Top_k%28y%2Cpre_y%29%3D%5Cbegin%7Bcases%7D%201%2C%20%7By%20%5Cin%20pre_y%7D%20%5C%5C%200%2C%20%7By%20%5Cnotin%20pre_y%7D%20%5Cend%7Bcases%7D)

其余指标见[参考文章](https://blog.csdn.net/fisherming/article/details/80209182)

### 机器学习参数调优
1. 网格搜索：穷举搜索，在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果
2. 随机搜索：与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。
3. 贝叶斯优化算法：贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。

### 尾声
时间来到现在，2020年，回过头望去，16年的那场战役，是一块里程碑，上面刻画了美好的蓝图，吸引着所有风投，所有科技公司，都在尽可能的尝试与人工智能挂钩，以获取更多的融资。可当国家的发展踩下了刹车，所有在飞的猪摔的稀烂，所有在裸泳的人接受世人嘲弄的眼光时，我想，人工智能将不再只是一个故事，这个行业也可以减少被资本裹挟，潜心发展。

