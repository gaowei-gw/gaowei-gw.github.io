# 机器学习综述
## 机器学习分类
* 1、监督学习：有输入和输出
* 2、非监督学习：有输入无输出
* 3、半监督学习：是监督学习和非监督学习结合
* 4、强化学习：学习策略已达到回报最大或实现特定目标
## 机器学习模型
机器学习 = 数据（data） + 模型（model） + 优化方法（optimal strategy）
## 机器学习算法导图
* 1、deep learning 深度学习
* 2、ensemble 集成学习
* 3、neural networks 神经网络学习
* 4、regularization 正则化
* 5、rule system 规则
* 6、regression 回归
* 7、bayesian 贝叶斯算法
* 8、decision tree 决策树
* 9、dimensionality reduction 降维
* 10、instance based 基于实例的学习
* 11、clustering 聚类
## 机器学习注意事项
* 1、相关并不意味着因果
* 2、可代表并不表明可学习
* 3、简单并不意味着精确
* 4、使用多个模型而不是一个
* 5、更多的数据打败更聪明的算法
* 6、如何获得训练用的特征是关键
* 7、learning的三大部分
* 8、模型好坏取决于其泛化能力
* 9、如果只有数据，那数据永远是不够的
* 10、overfitting问题(variance大) 方差大
* 11、高维情况下直觉不可信
* 12、理论上的保证不想你想的那样
## 常见的机器学习算法
* 1、Linear Algorithms  线性算法回归模型
* 2、Decision Tree 决策树
* 3、SVM 支持向量机
* 4、Naive Bayes Algorithms 朴素贝叶斯
* 5、kNN k近邻算法
* 6、Clustering Algorithms 聚类算法
* 7、K-Means K均值算法
* 8、Random Forest 随机森林
* 9、Dimensionality Reduction Algorithms 降维算法
* 10、Gradient Boosting algorithms 极度增强算法
* 11、Deep Learning Algorithms 深度学习算法
## [损失函数、代价函数、目标函数](https://www.cnblogs.com/lliuye/p/9549881.html)
损失函数（Loss Function）：是定义在单个样本上的，是指一个样本的误差。
代价函数（Cost Function）：是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均。
目标函数（Object Function）：是指最终需要优化的函数，一般来说是经验风险+结构风险，也就是（代价函数+正则化项）。
### 损失函数
##### （1）0-1损失函数（0-1 loss function） 
$$ L(y,f(x)) = \begin{cases} 0, & \text{y = f(x)} \\ 1, & \text{y $\neq$ f(x)} \end{cases} $$
##### （2）绝对值损失函数（absolute loss function）
$$ L(y,f(x))=|y-f(x)| $$
##### （3）平方损失函数（quadratic loss function）
$$ L(y,f(x))=(y-f(x))^2 $$
##### （4）log对数损失函数（logarithmic loss function）
$$ L(y,f(x))=log(1+e^{-yf(x)}) $$
##### （5）指数损失函数
$$ L(y,f(x))=exp(-yf(x)) $$
##### （6）Hinge损失函数
$$ L(w,b)=max\{0,1-yf(x)\} $$
### 混淆矩阵
混淆矩阵 | Predicted as Positive | Predicted as Negative
-|-|-
Labeled as Positive	| True Positive(TP) |	False Negative(FN)
Labeled as Negative | False Positive(FP)	| True Negative(TN)
* 假正越小 真负越大 
* 真正越大 假负越小
* 查准率和查全率是一对矛盾的度量

* ROC曲线的横轴为"假正率" 纵轴为"真正"
* AUC为ROC曲线下的面积
* 用AUC判断分类器（预测模型）优劣的标准：
* AUC = 1 为完美分类器
* 0.5 < AUC < 1 优于随机预测
* AUC < 0.5 比随机猜测还差；但只要总是反预测而行，就优于随机猜测.
